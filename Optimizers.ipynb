{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyH2m3ebbe2c"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "optimizer_vanilla = optim.SGD(model.parameters(), lr = 0.01)\n",
        "# Normal Gradient function\n",
        "\n",
        "optimizer_momentum = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
        "# Momentum Function uses friction control to speed up or slow down the step done by SGD high momentum  equals faster convergence\n",
        "\n",
        "optimizer_nesterov = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9, nesterov = True)\n",
        "# Nesterov does the same thing as momentum but stops at optimal point before reaching the end which may again go uphill"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0)\n",
        "# Adagrad also Adaptive Gradient will update weight based on parameter importances and change learning rate"
      ],
      "metadata": {
        "id": "xO5O9dIRf-_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def rms_prop_step(weight, grad, running_var, lr = 0.01, alpha = 0.99, eps = 1e-8):\n",
        "\n",
        "  new_var = (alpha * running_var) + ((1- alpha) * (grad ** 2))\n",
        "  # calculating new variance based on existing variance to keep learned parameters by giving alpha maximum value\n",
        "\n",
        "  step = (lr / math.sqrt(new_var + eps)) * grad\n",
        "  # Calculating gradient step to take\n",
        "\n",
        "  new_weight = weight - step\n",
        "\n",
        "  return new_weight, new_var"
      ],
      "metadata": {
        "id": "t5KK51ZLFz6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "alpha_decay = 0.9\n",
        "epsilon = 0.001\n",
        "\n",
        "current_weight = 10.0\n",
        "current_var = 0.0"
      ],
      "metadata": {
        "id": "JBiu4bgHxR_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_1 = 8.0\n",
        "\n",
        "current_weight, current_var = rms_prop_step(\n",
        "    weight = current_weight,\n",
        "    grad = gradient_1,\n",
        "    running_var = current_var,\n",
        "    lr = learning_rate,\n",
        "    alpha = alpha_decay,\n",
        "    eps = epsilon\n",
        ")"
      ],
      "metadata": {
        "id": "fm8sudyZMXTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"  -> New Variance (V): {current_var:.4f}\")\n",
        "print(f\"  -> New Weight:       {current_weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8mNs6QlOfdD",
        "outputId": "42407970-74dc-4363-a49c-906ff56c4aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> New Variance (V): 6.4000\n",
            "  -> New Weight:       9.6838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_2 = 1.0\n",
        "\n",
        "current_weight, current_var = rms_prop_step(\n",
        "    weight=current_weight,\n",
        "    grad=gradient_2,\n",
        "    running_var=current_var,\n",
        "    lr=learning_rate,\n",
        "    alpha=alpha_decay,\n",
        "    eps=epsilon\n",
        ")\n",
        "\n",
        "# The Learning rate will automatically adjusted and update weights relatively to previous update due to using alpha parameter"
      ],
      "metadata": {
        "id": "pziWdBQ3Of3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"  -> New Variance (V): {current_var:.4f}\")\n",
        "print(f\"  -> New Weight:       {current_weight:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYDqtD0XOvPk",
        "outputId": "ad3c11ec-4290-462b-a66b-2835f85d747b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> New Variance (V): 5.8600\n",
            "  -> New Weight:       9.6425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_step(weight, grad, m, v, t, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
        "\n",
        "  m_new = (beta1 * m) + ( (1- beta1) * grad)\n",
        "  # first calculate the momentum value to get the currently moving distance for the past time\n",
        "\n",
        "  v_new = (beta2 * v) + ( (1- beta2) * (grad ** 2))\n",
        "  # second calculate the variance value to get the average speed\n",
        "\n",
        "  m_hat = m_new / (1- beta1 ** t)\n",
        "  v_hat = v_new / (1 - beta2 ** t)\n",
        "\n",
        "  step = lr * m_hat / (math.sqrt(v_hat) + eps)\n",
        "  new_weight = weight - step\n",
        "\n",
        "  return new_weight, m_new, v_new\n"
      ],
      "metadata": {
        "id": "_hptYS51O38T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = 10.0\n",
        "m_hist = 0.0\n",
        "v_hist = 0.0\n",
        "step_count = 1\n",
        "gradient = 10.0\n",
        "\n",
        "w, m_hist, v_hist = adam_step(w, gradient, m_hist, v_hist, step_count)\n",
        "\n",
        "print(f\"Step 1 Momentum (m): {m_hist:.4f}\")\n",
        "print(f\"Step 1 Variance (v): {v_hist:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os_KIHvjUZ2d",
        "outputId": "2061f563-5835-4d60-bf5e-94750b9e9942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 Momentum (m): 1.0000\n",
            "Step 1 Variance (v): 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = 1.0\n",
        "step_count = 1\n",
        "gradient = 9.0\n",
        "\n",
        "w, m_hist, v_hist = adam_step(w, gradient, m_hist, v_hist, step_count)\n",
        "\n",
        "print(f\"Step 1 Momentum (m): {m_hist:.4f}\")\n",
        "print(f\"Step 1 Variance (v): {v_hist:.4f}\")\n",
        "\n",
        "# The ADAM optimizer same as combining momentum SGD and RMSProp by utilizing the momentun during step and also able to control the step and gives a good initial headstart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5NcXbdGUj1g",
        "outputId": "93ff5b4b-8442-4f4b-a80e-64aa2ed3e87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 Momentum (m): 1.8000\n",
            "Step 1 Variance (v): 0.1809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Nesterov Adam is same as ADAM with Nesterov calculation which combines past and current gradient to determine the step"
      ],
      "metadata": {
        "id": "hwZ9I8UfVBlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def adamw_step(weight, grad, m, v, t, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):\n",
        "\n",
        "    m_new = (beta1 * m) + ((1 - beta1) * grad)\n",
        "    v_new = (beta2 * v) + ((1 - beta2) * (grad ** 2))\n",
        "\n",
        "    m_hat = m_new / (1 - beta1 ** t)\n",
        "    v_hat = v_new / (1 - beta2 ** t)\n",
        "\n",
        "\n",
        "    adam_step = lr * (m_hat / (math.sqrt(v_hat) + eps))\n",
        "\n",
        "    decay_step = lr * weight_decay * weight\n",
        "    # Weight decay depend only on current weight and Learning Rate and ignores gradient step\n",
        "\n",
        "    new_weight = weight - adam_step - decay_step\n",
        "\n",
        "    return new_weight, m_new, v_new"
      ],
      "metadata": {
        "id": "7SeJ_2DFsl4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = 10.0\n",
        "grad = 0.0\n",
        "m = 0.0\n",
        "v = 0.0\n",
        "t = 1\n",
        "lr = 0.1\n",
        "decay = 0.1\n",
        "\n",
        "\n",
        "new_w, new_m, new_v = adamw_step(\n",
        "    w, grad, m, v, t,\n",
        "    lr=lr,\n",
        "    weight_decay=decay\n",
        ")\n",
        "\n",
        "print(\"ADAMW RESULT:\")\n",
        "print(f\"New Weight: {new_w:.4f}\")\n",
        "print(f\"Change:     {new_w - w:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez8bemw-s_73",
        "outputId": "90a19c4c-326c-47ea-f8eb-12f43ec6978c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADAMW RESULT:\n",
            "New Weight: 9.9000\n",
            "Change:     -0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1ciSSUntOb2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}